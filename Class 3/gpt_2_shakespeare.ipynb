{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "2075afec-4654-435e-f579-00bf87132e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=51082ce13982c66395236ee1e230e436ca3e3da588255b29fcd15690250f8177\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "25e7c424-d667-48fd-de13-85e02ca13126"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 261Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 3.43Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 598Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:12, 40.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 203Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 4.28Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 5.01Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "1a2873c9-2e06-484b-c7c0-1f4f6f809749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "41978740-7547-4658-c0df-6a03468eea0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "I don't know. I don't think I can understand that. I mean, I'm not saying it's a planet, but it's a planet with a planet. At the end of the day, we don't know what happened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "43e27b11-ac7f-484c-e2be-b1df8a1f2971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 13:57:05--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.216.216, 52.217.38.190, 54.231.231.72, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.216.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "\rnietzsche.txt         0%[                    ]       0  --.-KB/s               \rnietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-22 13:57:05 (23.8 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "c1ba7495-473f-408b-9e6d-a0075b6e8b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 185MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "b59755af-4d8a-4592-c71a-bee603432b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 15:19:13 (19.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "5b5b4a0c-b064-401b-cb30-f46dde3262cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 9.04] loss=3.50 avg=3.50\n",
            "[2 | 11.16] loss=3.41 avg=3.45\n",
            "[3 | 13.29] loss=3.42 avg=3.44\n",
            "[4 | 15.43] loss=3.36 avg=3.42\n",
            "[5 | 17.57] loss=3.52 avg=3.44\n",
            "[6 | 19.72] loss=3.29 avg=3.42\n",
            "[7 | 21.88] loss=3.15 avg=3.38\n",
            "[8 | 24.04] loss=3.24 avg=3.36\n",
            "[9 | 26.20] loss=3.20 avg=3.34\n",
            "[10 | 28.37] loss=3.16 avg=3.32\n",
            "[11 | 30.55] loss=3.23 avg=3.31\n",
            "[12 | 32.76] loss=3.33 avg=3.31\n",
            "[13 | 34.94] loss=3.25 avg=3.31\n",
            "[14 | 37.13] loss=3.15 avg=3.30\n",
            "[15 | 39.33] loss=3.10 avg=3.28\n",
            "[16 | 41.52] loss=3.19 avg=3.28\n",
            "[17 | 43.73] loss=3.07 avg=3.26\n",
            "[18 | 45.94] loss=3.22 avg=3.26\n",
            "[19 | 48.16] loss=3.01 avg=3.25\n",
            "[20 | 50.37] loss=3.22 avg=3.24\n",
            "[21 | 52.59] loss=3.10 avg=3.24\n",
            "[22 | 54.82] loss=3.07 avg=3.23\n",
            "[23 | 57.07] loss=3.23 avg=3.23\n",
            "[24 | 59.31] loss=3.12 avg=3.22\n",
            "[25 | 61.56] loss=3.20 avg=3.22\n",
            "[26 | 63.82] loss=3.09 avg=3.22\n",
            "[27 | 66.08] loss=3.07 avg=3.21\n",
            "[28 | 68.34] loss=3.04 avg=3.20\n",
            "[29 | 70.63] loss=3.06 avg=3.20\n",
            "[30 | 72.90] loss=3.10 avg=3.19\n",
            "[31 | 75.18] loss=2.95 avg=3.18\n",
            "[32 | 77.47] loss=2.97 avg=3.18\n",
            "[33 | 79.76] loss=2.93 avg=3.17\n",
            "[34 | 82.07] loss=3.00 avg=3.16\n",
            "[35 | 84.36] loss=2.99 avg=3.16\n",
            "[36 | 86.67] loss=3.00 avg=3.15\n",
            "[37 | 88.97] loss=2.97 avg=3.15\n",
            "[38 | 91.27] loss=3.08 avg=3.14\n",
            "[39 | 93.58] loss=2.96 avg=3.14\n",
            "[40 | 95.85] loss=2.99 avg=3.13\n",
            "[41 | 98.12] loss=3.03 avg=3.13\n",
            "[42 | 100.39] loss=2.93 avg=3.12\n",
            "[43 | 102.65] loss=2.85 avg=3.12\n",
            "[44 | 104.91] loss=3.02 avg=3.11\n",
            "[45 | 107.17] loss=3.11 avg=3.11\n",
            "[46 | 109.42] loss=3.03 avg=3.11\n",
            "[47 | 111.67] loss=2.98 avg=3.11\n",
            "[48 | 113.92] loss=3.11 avg=3.11\n",
            "[49 | 116.16] loss=2.80 avg=3.10\n",
            "[50 | 118.41] loss=2.98 avg=3.10\n",
            "[51 | 120.66] loss=2.97 avg=3.09\n",
            "[52 | 122.91] loss=2.96 avg=3.09\n",
            "[53 | 125.15] loss=2.95 avg=3.09\n",
            "[54 | 127.40] loss=3.00 avg=3.09\n",
            "[55 | 129.66] loss=3.04 avg=3.08\n",
            "[56 | 131.90] loss=2.98 avg=3.08\n",
            "[57 | 134.15] loss=2.91 avg=3.08\n",
            "[58 | 136.40] loss=2.82 avg=3.07\n",
            "[59 | 138.65] loss=3.00 avg=3.07\n",
            "[60 | 140.90] loss=2.96 avg=3.07\n",
            "[61 | 143.15] loss=2.83 avg=3.06\n",
            "[62 | 145.41] loss=2.99 avg=3.06\n",
            "[63 | 147.66] loss=2.87 avg=3.06\n",
            "[64 | 149.91] loss=2.86 avg=3.05\n",
            "[65 | 152.16] loss=3.00 avg=3.05\n",
            "[66 | 154.43] loss=3.06 avg=3.05\n",
            "[67 | 156.70] loss=2.89 avg=3.05\n",
            "[68 | 158.97] loss=2.89 avg=3.05\n",
            "[69 | 161.24] loss=2.81 avg=3.04\n",
            "[70 | 163.51] loss=2.94 avg=3.04\n",
            "[71 | 165.78] loss=2.85 avg=3.04\n",
            "[72 | 168.05] loss=3.05 avg=3.04\n",
            "[73 | 170.33] loss=2.91 avg=3.03\n",
            "[74 | 172.61] loss=2.78 avg=3.03\n",
            "[75 | 174.88] loss=2.87 avg=3.03\n",
            "[76 | 177.15] loss=2.73 avg=3.02\n",
            "[77 | 179.43] loss=2.95 avg=3.02\n",
            "[78 | 181.70] loss=2.89 avg=3.02\n",
            "[79 | 183.97] loss=2.97 avg=3.02\n",
            "[80 | 186.24] loss=3.07 avg=3.02\n",
            "[81 | 188.51] loss=2.84 avg=3.01\n",
            "[82 | 190.79] loss=2.75 avg=3.01\n",
            "[83 | 193.06] loss=2.93 avg=3.01\n",
            "[84 | 195.33] loss=2.91 avg=3.01\n",
            "[85 | 197.59] loss=2.85 avg=3.00\n",
            "[86 | 199.84] loss=2.76 avg=3.00\n",
            "[87 | 202.11] loss=2.81 avg=3.00\n",
            "[88 | 204.38] loss=2.86 avg=2.99\n",
            "[89 | 206.64] loss=2.79 avg=2.99\n",
            "[90 | 208.89] loss=2.80 avg=2.99\n",
            "[91 | 211.16] loss=2.75 avg=2.98\n",
            "[92 | 213.42] loss=2.87 avg=2.98\n",
            "[93 | 215.69] loss=2.87 avg=2.98\n",
            "[94 | 217.96] loss=2.79 avg=2.98\n",
            "[95 | 220.22] loss=2.97 avg=2.98\n",
            "[96 | 222.48] loss=2.82 avg=2.97\n",
            "[97 | 224.75] loss=2.78 avg=2.97\n",
            "[98 | 227.01] loss=2.80 avg=2.97\n",
            "[99 | 229.27] loss=2.78 avg=2.96\n",
            "[100 | 231.54] loss=2.90 avg=2.96\n",
            "======== SAMPLE 1 ========\n",
            " to the left, to the right, and onto the street. What was he thinking of?\" She looked around to see if anything was amiss. \"When he asked \n",
            "about the girl and her name, I did not hear. Nor was it to my chagrin as he said he had no idea. He \n",
            "had promised me that I should see her first, and that's what I told him.\" \n",
            "Ser Barristan Selmy saw the boy and knew very well what was so wrong. \"When you call \n",
            "her that, you're not calling me that, are you? \"He said, glancing down at his own black eyes. \"When you call \n",
            "her that, you're not calling me that either, you're calling her that.\" \n",
            "The boy sighed in relief. \"I was never truly certain,\" he said quietly. \"My \n",
            "mind was always full of doubts.\" \n",
            "Page 561\n",
            "\n",
            "Lord Hoster and I had known each other for some time. Father had had him put in for a \n",
            "swift rebuke when Lord Mormont forbade him to visit the castle. He asked that his brother Ser \n",
            "Mordane be sent home, only that he should sit on the bench where she stood between us, but Lord \n",
            "Hoster had been quick to let it be said he meant to hold his brother's head. \"The \n",
            "Lady of Flowers, \n",
            "your honor, I would like to come with you,\" he had said, nodding. \n",
            "\"I will have no trouble,\" Lord Hoster answered, \"but if it may be so please you to take the leave of \n",
            "My Grace and take the reins of my lord son, I need to know \n",
            "what you will find my father. A tourney has been declared by the queen, so we are expected to march \n",
            "along with her and the royal household.\" \n",
            "Ser Barristan shrugged. \"Thank you, I have not forgotten.\" \n",
            "\"You are not welcome to tourney, Ser Barristan,\" Lady Cersei interrupted, smiling. \"You do \n",
            "not want to join me.\" \n",
            "Sansa Lannister was more than a bit disappointed. She was a beautiful girl indeed, but her face was \n",
            "not like any girl's. In her heart, she was nothing. Sansa had never felt so ugly, \n",
            "no more so than she did at the queen. \n",
            "Afterward she was taken to the window of a castle, naked, covered with dirt. Ser Youn Harkonnen \n",
            "had promised her that he would give her a bedroll before she was allowed under her clothing. Ser Barristan \n",
            "had been so intent that he gave her the first blow whenever she tried to take it. He told her that no man \n",
            "would be able to take off and keep it off. When he brought her to the room, he gave her the word, \n",
            "and he gave her his hand. \"A bedroll, babe?\" he asked her. \n",
            "That night when she stood naked before him in the yard of their home, she heard the words. \n",
            "\"Your father was a king. Who was to answer to your mother or uncle, queen?\" \n",
            "Ser Barristan shrugged. \n",
            "\"Your father,\" she replied. \"No one was in his place.\" \n",
            "\"Do you remember?\" Ser Barristan asked. \n",
            "\"Remember it all, boy.\" \n",
            "\"The moment he left his seat,\" she said, \"I was the first to fall.\" \n",
            "\"I fell first,\" Ser Barristan said. \"You and I were first to fall, before Robert and I. The \n",
            "great stone fell and we're both stillborn now. There are still some who say that you were \n",
            "too young for that.\" \n",
            "\"If you did, I wouldn't have it any other way.\" \n",
            "\"I'm not old enough for that,\" she confessed. \n",
            "\"If you were, you've still got a name, don't you?\" \n",
            "\"Yes,\" she said. \n",
            "He saw her face. \"I'm not in there to hold your baby.\" \n",
            "\"You can.\" \n",
            "\"Oh, you're the youngest of them, maester,\" her brother replied. \n",
            "\"Who makes the decisions?\" Ser Barristan asked suspiciously. \"I'm your brother,\" he \n",
            "said. \"That's the way I do things, not my name . . . but you've been a king long enough, \n",
            "that it makes no difference to you, you've done what you believed as a man . . . no \n",
            "place else that I've ever known. That you know how it feels to fall and you remember it \n",
            "well. \n",
            "Page 562\n",
            "\n",
            "\"My father was a king.\" He glanced at the girl in the windows of his house\n",
            "\n",
            "[101 | 246.41] loss=2.88 avg=2.96\n",
            "[102 | 248.67] loss=2.80 avg=2.96\n",
            "[103 | 250.94] loss=2.89 avg=2.96\n",
            "[104 | 253.20] loss=2.82 avg=2.96\n",
            "[105 | 255.45] loss=2.78 avg=2.95\n",
            "[106 | 257.72] loss=2.72 avg=2.95\n",
            "[107 | 259.99] loss=2.76 avg=2.95\n",
            "[108 | 262.26] loss=2.83 avg=2.95\n",
            "[109 | 264.53] loss=2.70 avg=2.94\n",
            "[110 | 266.79] loss=2.81 avg=2.94\n",
            "[111 | 269.06] loss=2.64 avg=2.94\n",
            "[112 | 271.33] loss=2.86 avg=2.93\n",
            "[113 | 273.60] loss=2.86 avg=2.93\n",
            "[114 | 275.88] loss=2.80 avg=2.93\n",
            "[115 | 278.15] loss=2.85 avg=2.93\n",
            "[116 | 280.42] loss=2.66 avg=2.93\n",
            "[117 | 282.69] loss=2.86 avg=2.92\n",
            "[118 | 284.96] loss=2.73 avg=2.92\n",
            "[119 | 287.23] loss=2.84 avg=2.92\n",
            "[120 | 289.50] loss=2.76 avg=2.92\n",
            "[121 | 291.78] loss=2.84 avg=2.92\n",
            "[122 | 294.03] loss=2.69 avg=2.91\n",
            "[123 | 296.30] loss=2.84 avg=2.91\n",
            "[124 | 298.58] loss=2.68 avg=2.91\n",
            "[125 | 300.86] loss=2.65 avg=2.91\n",
            "[126 | 303.14] loss=2.69 avg=2.90\n",
            "[127 | 305.41] loss=2.61 avg=2.90\n",
            "[128 | 307.68] loss=2.77 avg=2.90\n",
            "[129 | 309.96] loss=2.57 avg=2.89\n",
            "[130 | 312.24] loss=2.87 avg=2.89\n",
            "[131 | 314.51] loss=2.80 avg=2.89\n",
            "[132 | 316.78] loss=2.71 avg=2.89\n",
            "[133 | 319.06] loss=2.67 avg=2.89\n",
            "[134 | 321.33] loss=2.69 avg=2.88\n",
            "[135 | 323.60] loss=2.56 avg=2.88\n",
            "[136 | 325.88] loss=2.79 avg=2.88\n",
            "[137 | 328.15] loss=2.59 avg=2.87\n",
            "[138 | 330.41] loss=2.70 avg=2.87\n",
            "[139 | 332.68] loss=2.81 avg=2.87\n",
            "[140 | 334.96] loss=2.74 avg=2.87\n",
            "[141 | 337.23] loss=2.75 avg=2.87\n",
            "[142 | 339.50] loss=2.78 avg=2.87\n",
            "[143 | 341.77] loss=2.66 avg=2.86\n",
            "[144 | 344.03] loss=2.89 avg=2.86\n",
            "[145 | 346.30] loss=2.98 avg=2.87\n",
            "[146 | 348.57] loss=2.83 avg=2.87\n",
            "[147 | 350.84] loss=2.90 avg=2.87\n",
            "[148 | 353.10] loss=2.65 avg=2.86\n",
            "[149 | 355.37] loss=2.68 avg=2.86\n",
            "[150 | 357.64] loss=2.78 avg=2.86\n",
            "[151 | 359.92] loss=2.63 avg=2.86\n",
            "[152 | 362.19] loss=2.57 avg=2.85\n",
            "[153 | 364.45] loss=2.67 avg=2.85\n",
            "[154 | 366.72] loss=2.57 avg=2.85\n",
            "[155 | 368.99] loss=2.65 avg=2.84\n",
            "[156 | 371.26] loss=2.62 avg=2.84\n",
            "[157 | 373.53] loss=2.83 avg=2.84\n",
            "[158 | 375.79] loss=2.77 avg=2.84\n",
            "[159 | 378.06] loss=2.78 avg=2.84\n",
            "[160 | 380.33] loss=2.61 avg=2.84\n",
            "[161 | 382.60] loss=2.54 avg=2.83\n",
            "[162 | 384.87] loss=2.83 avg=2.83\n",
            "[163 | 387.13] loss=2.48 avg=2.83\n",
            "[164 | 389.40] loss=2.69 avg=2.83\n",
            "[165 | 391.67] loss=2.84 avg=2.83\n",
            "[166 | 393.94] loss=2.73 avg=2.83\n",
            "[167 | 396.20] loss=2.65 avg=2.82\n",
            "[168 | 398.47] loss=2.55 avg=2.82\n",
            "[169 | 400.74] loss=2.58 avg=2.82\n",
            "[170 | 403.01] loss=2.65 avg=2.82\n",
            "[171 | 405.28] loss=2.84 avg=2.82\n",
            "[172 | 407.56] loss=2.56 avg=2.81\n",
            "[173 | 409.84] loss=2.57 avg=2.81\n",
            "[174 | 412.11] loss=2.77 avg=2.81\n",
            "[175 | 414.38] loss=2.60 avg=2.81\n",
            "[176 | 416.66] loss=2.78 avg=2.81\n",
            "[177 | 418.93] loss=2.61 avg=2.80\n",
            "[178 | 421.21] loss=2.82 avg=2.80\n",
            "[179 | 423.49] loss=2.74 avg=2.80\n",
            "[180 | 425.76] loss=2.48 avg=2.80\n",
            "[181 | 428.03] loss=2.54 avg=2.80\n",
            "[182 | 430.30] loss=2.59 avg=2.79\n",
            "[183 | 432.58] loss=2.72 avg=2.79\n",
            "[184 | 434.86] loss=2.58 avg=2.79\n",
            "[185 | 437.13] loss=2.74 avg=2.79\n",
            "[186 | 439.40] loss=2.53 avg=2.79\n",
            "[187 | 441.66] loss=2.59 avg=2.78\n",
            "[188 | 443.93] loss=2.64 avg=2.78\n",
            "[189 | 446.21] loss=2.76 avg=2.78\n",
            "[190 | 448.48] loss=2.52 avg=2.78\n",
            "[191 | 450.75] loss=2.55 avg=2.78\n",
            "[192 | 453.02] loss=2.71 avg=2.78\n",
            "[193 | 455.28] loss=2.63 avg=2.77\n",
            "[194 | 457.56] loss=2.55 avg=2.77\n",
            "[195 | 459.82] loss=2.50 avg=2.77\n",
            "[196 | 462.09] loss=2.36 avg=2.76\n",
            "[197 | 464.36] loss=2.47 avg=2.76\n",
            "[198 | 466.62] loss=2.56 avg=2.76\n",
            "[199 | 468.89] loss=2.60 avg=2.76\n",
            "[200 | 471.15] loss=2.29 avg=2.75\n",
            "======== SAMPLE 1 ========\n",
            " had been a longtime friend of mine. And I \n",
            "found a new wife here in the Free City called Lady Lothor, and they are going to take me back to King Eddard and the \n",
            "Dornish Kingdoms in \n",
            "Winterfell. If you'd like to see my mail once in a while, feel free.\" \n",
            "Robb shook his head helplessly from the mention of it, not so much to look on Dany's \n",
            "face or Mormont's laughter, but to ignore it. He had not said a word, so it was no big deal; he \n",
            "had only wished that Robb could speak. \n",
            "\"Do you have any children?\" Ser Jaremy asked anxiously. \n",
            "\"No, my lord,\" Lord Stannis reminded him. \"Only the children. I want them to grow up good by my side and \n",
            "not so much the other way.\" \n",
            "\"I know why you wanted them,\" Ser Jaremy said. \"You had your own reasons. It's not \n",
            "my place to make decisions for you.\" \n",
            "\"It was your sister, old Ser Mervyn,\" Dany blurted, but he would not laugh. \"I thought that meant you \n",
            "had to stay.\" \n",
            "\"Even if you wouldn't,\" Lord Stannis said. \"Why would you want to? You love her and she has always \n",
            "been faithful, and when I was old enough to die her death, and she will never do that?\" \n",
            "\"If she never does, I'll have no choice but to take her to the north,\" Sansa said grimly. \n",
            "Sansa had been at her wits' end. \"If I thought . . . we should go, I would rather that I did not have to \n",
            "live here. I wish to be a knight, to lead a household, to marry and take care of children, but I need all \n",
            "of you, and you and your sister. No, I won't have that. You can still help me as well, though. If you \n",
            "were ever the child of Winterfell, you might want to come with us.\" \n",
            "\"You ought to be getting out of the city, m1ord.\" Sansa felt a little silly now. \"Yes, m1ord. I \n",
            "wanted it, and now you want it, and now you want it, m1ord.\" \n",
            "Sansa made it as she spoke, but Joffrey took the opportunity to make her laugh even louder. \"Lord Stark will \n",
            "be leaving without us,\" he said, smiling. \"Let me see, m1ord. I'd been thinking of going to see \n",
            "King's Landing with your sister.\" \n",
            "\"I'm going too, m1ord. I know you will be too. I want you.\" Sansa could not leave him. \n",
            "Page 12\n",
            "\n",
            "\"Go,\" Lord Stannis cut her away. \"No, please, no.\" \n",
            "Sansa did not want him there. \"Lord Eddard,\" she begged his forgiveness, \"I thought you would like that, you know \n",
            "it. I can hear you saying it, m1ord, or else . . . you are no longer the knight you was, I am. \n",
            "I have paid a debt. You owe me.\" \n",
            "'I have paid a debt,\" Stark said. \n",
            "Sansa watched them for a moment, the queen struggling with anger. She had not thought of Jaime, yet she \n",
            "did. \"I have paid a debt,\" she said, and that was all. \"You don't have to repay me. My lord, if you \n",
            "believe me you must repay your brother.\" \"No, I haven't,\" she answered, but he had. \"I promise I would.\" \n",
            "Lord Eddard smiled. \"What did you hear?\" \n",
            "\"I asked no questions, m1ord,\" she told him. \"No one has ever said a word in \n",
            "Page 13\n",
            "\n",
            "the Old Town. This man has gone mad with his money, m1ord. When he comes to see you, he will take a word, and m1ord, \n",
            "he will laugh, and they will know it for himself. He will take my daughter and his daughter's daughter and everything in his \n",
            "hands, and if you believe me he has all the gold and the glory of every man he will need in the kingsroad, he will \n",
            "take my daughter, m1ord. That is all.\" \n",
            "It was not good enough. Sansa had sworn an oath to protect the king, to protect the king's lady \n",
            "Lannister and their sons. That would be treason, she knew it all would be treason. To repay her sister was \n",
            "not the same as to repay her father. \n",
            "\n",
            "[201 | 484.66] loss=2.61 avg=2.75\n",
            "[202 | 486.92] loss=2.54 avg=2.75\n",
            "[203 | 489.18] loss=2.51 avg=2.74\n",
            "[204 | 491.44] loss=2.75 avg=2.74\n",
            "[205 | 493.71] loss=2.29 avg=2.74\n",
            "[206 | 495.97] loss=2.40 avg=2.74\n",
            "[207 | 498.23] loss=2.59 avg=2.73\n",
            "[208 | 500.49] loss=2.71 avg=2.73\n",
            "[209 | 502.74] loss=2.66 avg=2.73\n",
            "[210 | 505.00] loss=2.67 avg=2.73\n",
            "[211 | 507.26] loss=2.46 avg=2.73\n",
            "[212 | 509.52] loss=2.70 avg=2.73\n",
            "[213 | 511.77] loss=2.39 avg=2.72\n",
            "[214 | 514.04] loss=2.30 avg=2.72\n",
            "[215 | 516.31] loss=2.42 avg=2.72\n",
            "[216 | 518.58] loss=2.56 avg=2.71\n",
            "[217 | 520.84] loss=2.58 avg=2.71\n",
            "[218 | 523.11] loss=2.78 avg=2.71\n",
            "[219 | 525.38] loss=2.39 avg=2.71\n",
            "[220 | 527.63] loss=2.41 avg=2.71\n",
            "[221 | 529.90] loss=2.37 avg=2.70\n",
            "[222 | 532.16] loss=2.53 avg=2.70\n",
            "[223 | 534.43] loss=2.36 avg=2.70\n",
            "[224 | 536.68] loss=2.41 avg=2.69\n",
            "[225 | 538.95] loss=2.49 avg=2.69\n",
            "[226 | 541.22] loss=2.80 avg=2.69\n",
            "[227 | 543.48] loss=2.74 avg=2.69\n",
            "[228 | 545.75] loss=2.52 avg=2.69\n",
            "[229 | 548.02] loss=2.42 avg=2.69\n",
            "[230 | 550.28] loss=2.61 avg=2.69\n",
            "[231 | 552.55] loss=2.34 avg=2.68\n",
            "[232 | 554.82] loss=2.55 avg=2.68\n",
            "[233 | 557.08] loss=2.71 avg=2.68\n",
            "[234 | 559.33] loss=2.67 avg=2.68\n",
            "[235 | 561.59] loss=2.43 avg=2.68\n",
            "[236 | 563.85] loss=2.52 avg=2.68\n",
            "[237 | 566.12] loss=2.38 avg=2.67\n",
            "[238 | 568.38] loss=2.45 avg=2.67\n",
            "[239 | 570.63] loss=2.64 avg=2.67\n",
            "[240 | 572.89] loss=2.63 avg=2.67\n",
            "[241 | 575.15] loss=2.44 avg=2.67\n",
            "[242 | 577.42] loss=2.49 avg=2.67\n",
            "[243 | 579.69] loss=2.44 avg=2.66\n",
            "[244 | 581.95] loss=2.50 avg=2.66\n",
            "[245 | 584.22] loss=2.55 avg=2.66\n",
            "[246 | 586.49] loss=2.35 avg=2.66\n",
            "[247 | 588.76] loss=2.19 avg=2.65\n",
            "[248 | 591.04] loss=2.37 avg=2.65\n",
            "[249 | 593.29] loss=2.14 avg=2.64\n",
            "[250 | 595.56] loss=2.43 avg=2.64\n",
            "[251 | 597.83] loss=2.64 avg=2.64\n",
            "[252 | 600.10] loss=2.47 avg=2.64\n",
            "[253 | 602.38] loss=2.55 avg=2.64\n",
            "[254 | 604.65] loss=2.52 avg=2.64\n",
            "[255 | 606.92] loss=2.37 avg=2.63\n",
            "[256 | 609.19] loss=2.56 avg=2.63\n",
            "[257 | 611.46] loss=2.49 avg=2.63\n",
            "[258 | 613.73] loss=2.23 avg=2.63\n",
            "[259 | 616.00] loss=2.43 avg=2.63\n",
            "[260 | 618.26] loss=2.63 avg=2.63\n",
            "[261 | 620.53] loss=2.60 avg=2.63\n",
            "[262 | 622.80] loss=2.36 avg=2.62\n",
            "[263 | 625.07] loss=2.41 avg=2.62\n",
            "[264 | 627.35] loss=2.48 avg=2.62\n",
            "[265 | 629.63] loss=2.64 avg=2.62\n",
            "[266 | 631.90] loss=2.17 avg=2.61\n",
            "[267 | 634.17] loss=2.27 avg=2.61\n",
            "[268 | 636.44] loss=2.20 avg=2.61\n",
            "[269 | 638.71] loss=2.40 avg=2.60\n",
            "[270 | 640.99] loss=2.25 avg=2.60\n",
            "[271 | 643.27] loss=2.50 avg=2.60\n",
            "[272 | 645.54] loss=2.35 avg=2.60\n",
            "[273 | 647.81] loss=2.20 avg=2.59\n",
            "[274 | 650.08] loss=2.18 avg=2.59\n",
            "[275 | 652.36] loss=2.14 avg=2.58\n",
            "[276 | 654.63] loss=2.27 avg=2.58\n",
            "[277 | 656.90] loss=2.19 avg=2.58\n",
            "[278 | 659.17] loss=2.46 avg=2.57\n",
            "[279 | 661.43] loss=2.45 avg=2.57\n",
            "[280 | 663.71] loss=2.41 avg=2.57\n",
            "[281 | 665.97] loss=2.52 avg=2.57\n",
            "[282 | 668.23] loss=2.41 avg=2.57\n",
            "[283 | 670.51] loss=2.28 avg=2.57\n",
            "[284 | 672.78] loss=2.34 avg=2.56\n",
            "[285 | 675.04] loss=2.29 avg=2.56\n",
            "[286 | 677.32] loss=2.33 avg=2.56\n",
            "[287 | 679.58] loss=2.12 avg=2.55\n",
            "[288 | 681.84] loss=2.46 avg=2.55\n",
            "[289 | 684.11] loss=2.24 avg=2.55\n",
            "[290 | 686.37] loss=2.16 avg=2.55\n",
            "[291 | 688.64] loss=2.43 avg=2.54\n",
            "[292 | 690.91] loss=2.17 avg=2.54\n",
            "[293 | 693.17] loss=2.55 avg=2.54\n",
            "[294 | 695.43] loss=2.20 avg=2.54\n",
            "[295 | 697.70] loss=2.49 avg=2.54\n",
            "[296 | 699.97] loss=2.65 avg=2.54\n",
            "[297 | 702.24] loss=2.29 avg=2.54\n",
            "[298 | 704.50] loss=2.42 avg=2.53\n",
            "[299 | 706.76] loss=2.36 avg=2.53\n",
            "[300 | 709.03] loss=2.24 avg=2.53\n",
            "======== SAMPLE 1 ========\n",
            " one said her name. \n",
            "Her eyes watched as a small woman ran to the balcony with their \n",
            "bud bath water. That was as good a moment as any. \n",
            "When Prince Joffrey returned to the royal apartments, King Robert sat on the small terrace above a \n",
            "bannerswirl, his huge head bowed and his cheeks flushed. \"My lady and I were fortunate enough to see \n",
            "him at first sight.\" \n",
            "\"That boy,\" Ned began. \"No doubt he is the heir.\" \n",
            "Robert nodded. \"The king would love to see you again.\" His eyes were dark and fearful. Varys and \n",
            "Page 478\n",
            "\n",
            "Todder were laughing as they watched the king grow bald and brittle. \"My lady, he is still young . . .\" \n",
            "\"A year old . . . and his eyes . . . well . . .\" Ned began to explain, but Varys \n",
            "did not believe him. \"They look very old. And he is still the father, I suspect. A little man for a prince . . . \n",
            "\"My lady, that boy was never quite what I should call himself. His father was a high lord lord in \n",
            "Kingsguard. And some say his father was Lord of the Eyrie. It is said that Lord of the Eyrie was his \n",
            "father's brother.\" \n",
            "\"I will not lie to you,\" Ned said. \"He must have known what a great man that was.\" It came as \n",
            "clear as day to Jory and Ser Barristan. They had seen that face many times in their lives, yet Jory \n",
            "was different. \"What did Lady Stark mean?\" he demanded. \n",
            "\"By treason.\" \n",
            "\"Yes,\" Ned replied. \n",
            "\"A great lord has many men of the Hand, if not some great lady,\" replied Ser Stannis. \"A great lady . . . that \n",
            "loves to fight.\" He lowered his eyes. Ned's brow was a knot of fur. \n",
            "For a moment, he did not believe what he was saying. He had never been a great lord. His father \n",
            "had been a lord of the Eyrie, a Grand Maester of the East. Ned felt a sudden sense of dread as the words \n",
            "lameged their way through his mouth. Varys was the oldest of these two sons of Lannister. Ned \n",
            "was the youngest. He had no time for any kind of humor. The notion \n",
            "that the boy had been brought up with such a close friendship was abhorrent. Ned could not \n",
            "stand to hear that anyone talk of their son. No, he was sorry he had not loved them . . . yet it \n",
            "was only a matter of time before he was angry. \n",
            "\"I want justice for Stannis!\" \n",
            "A few moments of silence followed. Jonos and Barristan looked up from their books. Arya rose from her \n",
            "bed, and Tyrion looked down to the floor. Her silver-gold eyes filled up with amusement. \n",
            "\"Do you have food yet?\" she asked. \n",
            "\"Yes, thanks for your help.\" \n",
            "\"What are you going to eat?\" he asked. \n",
            "\"I am going to bring Bran back for supper,\" she said, but first he must make room for his raven-haired brother \n",
            "to come around. He gave her the direwolf bow and brought Bran to her, with Robb beside him. \n",
            "They had come so long to ride this perilous road, but Jonos had no fear. Even when Robb was \n",
            "with him, he could feel her eyes on his face, as he watched him. He had heard stories about Lady \n",
            "Lannister. That was not true. He knew what Robb and Renly and Ser Rodrik had done to Arya. The \n",
            "Page 479\n",
            "\n",
            "castle had been theirs all along. It had been her womb, his own; he and Ser Rodrik had driven it up the wall \n",
            "until they had brought her out with the ravens. Robb and Renly had brought her to him as well; the \n",
            "Page 480\n",
            "\n",
            "woman had been his ward. She was not his wife. \n",
            "The king had left the chamber to return to his chambers, with a cup of wine. Lord Tywin wore a \n",
            "banner of crimson velvet over his head, and his sons rode with him in his armored cloak. \n",
            "Jonos and Rickon were still seated, surrounded by his sons. When Robb returned \n",
            "by his chambers, Jonos placed a hand on his belly. \"I will not leave them standing. They must go.\" \n",
            "Robb laid his head down in a soft towel, while Tyrion carried Bran to the king. \n",
            "\"Is Bran ready to go?\" Robb asked. \n",
            "\"Yes, lad,\" she said. \"\n",
            "\n",
            "[301 | 722.77] loss=2.28 avg=2.53\n",
            "[302 | 725.04] loss=2.21 avg=2.52\n",
            "[303 | 727.30] loss=2.10 avg=2.52\n",
            "[304 | 729.57] loss=2.56 avg=2.52\n",
            "[305 | 731.84] loss=2.12 avg=2.51\n",
            "[306 | 734.11] loss=2.07 avg=2.51\n",
            "[307 | 736.39] loss=1.98 avg=2.50\n",
            "[308 | 738.65] loss=2.04 avg=2.50\n",
            "[309 | 740.91] loss=2.27 avg=2.50\n",
            "[310 | 743.18] loss=2.31 avg=2.50\n",
            "[311 | 745.45] loss=2.31 avg=2.49\n",
            "[312 | 747.72] loss=2.42 avg=2.49\n",
            "[313 | 749.99] loss=2.25 avg=2.49\n",
            "[314 | 752.25] loss=2.13 avg=2.49\n",
            "[315 | 754.52] loss=2.28 avg=2.48\n",
            "[316 | 756.79] loss=2.43 avg=2.48\n",
            "[317 | 759.07] loss=2.20 avg=2.48\n",
            "[318 | 761.36] loss=2.32 avg=2.48\n",
            "[319 | 763.63] loss=2.14 avg=2.48\n",
            "[320 | 765.89] loss=2.34 avg=2.47\n",
            "[321 | 768.16] loss=2.33 avg=2.47\n",
            "[322 | 770.43] loss=2.16 avg=2.47\n",
            "[323 | 772.70] loss=2.12 avg=2.47\n",
            "[324 | 774.97] loss=2.30 avg=2.46\n",
            "[325 | 777.25] loss=2.05 avg=2.46\n",
            "[326 | 779.52] loss=2.21 avg=2.46\n",
            "[327 | 781.80] loss=2.06 avg=2.45\n",
            "[328 | 784.07] loss=2.18 avg=2.45\n",
            "[329 | 786.35] loss=2.16 avg=2.45\n",
            "[330 | 788.63] loss=2.15 avg=2.44\n",
            "[331 | 790.91] loss=2.15 avg=2.44\n",
            "[332 | 793.18] loss=2.69 avg=2.44\n",
            "[333 | 795.46] loss=2.43 avg=2.44\n",
            "[334 | 797.74] loss=2.06 avg=2.44\n",
            "[335 | 800.01] loss=2.22 avg=2.44\n",
            "[336 | 802.28] loss=2.19 avg=2.43\n",
            "[337 | 804.55] loss=2.26 avg=2.43\n",
            "[338 | 806.82] loss=1.99 avg=2.43\n",
            "[339 | 809.10] loss=2.25 avg=2.43\n",
            "[340 | 811.37] loss=1.93 avg=2.42\n",
            "[341 | 813.65] loss=2.22 avg=2.42\n",
            "[342 | 815.92] loss=2.25 avg=2.42\n",
            "[343 | 818.19] loss=2.35 avg=2.42\n",
            "[344 | 820.47] loss=2.19 avg=2.41\n",
            "[345 | 822.74] loss=2.10 avg=2.41\n",
            "[346 | 825.02] loss=2.07 avg=2.41\n",
            "[347 | 827.29] loss=2.24 avg=2.41\n",
            "[348 | 829.56] loss=2.52 avg=2.41\n",
            "[349 | 831.82] loss=2.24 avg=2.41\n",
            "[350 | 834.11] loss=2.19 avg=2.40\n",
            "[351 | 836.38] loss=2.15 avg=2.40\n",
            "[352 | 838.65] loss=2.14 avg=2.40\n",
            "[353 | 840.92] loss=2.26 avg=2.40\n",
            "[354 | 843.19] loss=2.20 avg=2.39\n",
            "[355 | 845.46] loss=2.27 avg=2.39\n",
            "[356 | 847.73] loss=2.23 avg=2.39\n",
            "[357 | 850.01] loss=2.23 avg=2.39\n",
            "[358 | 852.28] loss=2.19 avg=2.39\n",
            "[359 | 854.55] loss=2.11 avg=2.38\n",
            "[360 | 856.83] loss=2.39 avg=2.38\n",
            "[361 | 859.11] loss=1.91 avg=2.38\n",
            "[362 | 861.38] loss=1.98 avg=2.38\n",
            "[363 | 863.66] loss=2.14 avg=2.37\n",
            "[364 | 865.93] loss=2.12 avg=2.37\n",
            "[365 | 868.21] loss=2.18 avg=2.37\n",
            "[366 | 870.49] loss=2.36 avg=2.37\n",
            "[367 | 872.76] loss=2.23 avg=2.37\n",
            "[368 | 875.03] loss=2.51 avg=2.37\n",
            "[369 | 877.31] loss=1.87 avg=2.36\n",
            "[370 | 879.58] loss=2.00 avg=2.36\n",
            "[371 | 881.86] loss=2.18 avg=2.36\n",
            "[372 | 884.13] loss=2.65 avg=2.36\n",
            "[373 | 886.41] loss=1.94 avg=2.36\n",
            "[374 | 888.68] loss=2.28 avg=2.36\n",
            "[375 | 890.95] loss=2.22 avg=2.35\n",
            "[376 | 893.23] loss=2.10 avg=2.35\n",
            "[377 | 895.50] loss=2.10 avg=2.35\n",
            "[378 | 897.77] loss=2.08 avg=2.35\n",
            "[379 | 900.04] loss=2.35 avg=2.35\n",
            "[380 | 902.31] loss=2.02 avg=2.34\n",
            "[381 | 904.58] loss=2.21 avg=2.34\n",
            "[382 | 906.85] loss=1.99 avg=2.34\n",
            "[383 | 909.12] loss=2.32 avg=2.34\n",
            "[384 | 911.39] loss=2.30 avg=2.34\n",
            "[385 | 913.66] loss=1.98 avg=2.33\n",
            "[386 | 915.93] loss=1.99 avg=2.33\n",
            "[387 | 918.20] loss=2.07 avg=2.33\n",
            "[388 | 920.48] loss=2.48 avg=2.33\n",
            "[389 | 922.75] loss=2.26 avg=2.33\n",
            "[390 | 925.02] loss=1.91 avg=2.32\n",
            "[391 | 927.29] loss=2.27 avg=2.32\n",
            "[392 | 929.57] loss=2.11 avg=2.32\n",
            "[393 | 931.84] loss=2.02 avg=2.32\n",
            "[394 | 934.11] loss=2.09 avg=2.32\n",
            "[395 | 936.39] loss=1.83 avg=2.31\n",
            "[396 | 938.66] loss=1.75 avg=2.31\n",
            "[397 | 940.93] loss=2.16 avg=2.30\n",
            "[398 | 943.20] loss=2.09 avg=2.30\n",
            "[399 | 945.47] loss=1.96 avg=2.30\n",
            "[400 | 947.74] loss=2.06 avg=2.30\n",
            "======== SAMPLE 1 ========\n",
            " 5 \n",
            "\"I told him about the other one, m1ord, about the boy \n",
            "who carried the torch for \n",
            "Lysa. Another woman with the same surname. I thought I knew her, yet \n",
            "I was wrong. The boy was no friend to the boy he had fathered, and there was a girl of \n",
            "Torrhen's who was nine . . .\" \n",
            "Robb finally understood. \"The girl,\" he began, \"who had no friends . . .\" He glanced over \n",
            "Ned's face and gasped. \"And there was a m1ord?\" \n",
            "\"No one to speak to,\" Ned said. \n",
            "\"You're not supposed to speak to me.\" \n",
            "\"No one to speak to?\" \n",
            "Robb snorted. \"Did you spot a moment ago?\" \n",
            "Page 553\n",
            "\n",
            "\"In the street. No one was looking, but I heard a noise. \n",
            "\"Robb, see what I mean. The girl was with him with the torch on.\" \n",
            "Ned fell silent. \"I knew it was you. There was a m1ord in the street. I'd heard voices. It was \n",
            "Page 554\n",
            "\n",
            "Maester Aemon.\" \n",
            "Robb looked at him strangely. \"Maester Aemon, how long is the letter?\" \n",
            "Ned could feel a deep unease, something about this man's look, or the way he seemed to move, or perhaps even \n",
            "flinch. \n",
            "\"All I know is that he was with your son in Vaes Dothrak and the girl as they were crossing the Tully \n",
            "River.\" \n",
            "\"He might have,\" Ned said. \"The Hand said he was with Ser Jorah.\" \n",
            "Ned looked at the king with the king's eyes. \"So this Hand, what is his name?\" \n",
            "\"The Hand of the King.\" \n",
            "Ned's heart raced. \"How?\" \n",
            "\"Sansa, take the letter, and hold it with her hand.\" \n",
            "DAENERYS \n",
            "Septa Mordane held it up for the queen. The maester's seal was wed to his mail. \n",
            "\"Your Grace,\" Queen Cersei said, \"that letter I took from Ser Jorah Mormont in the crypt beneath the \n",
            "Dragonpit. I have this great missus whose true name I do not know, but it must have been sent to \n",
            "Prince Joffrey of the Eyrie from somewhere else in the solar system. He was a direwolf of fourteen years \n",
            ". He was at rest on the direwolf king's great kennel when I opened it, and a hundred other direwolves, in three \n",
            "years.\" \n",
            "Eddard Stark looked at her. \"A hundred other direwolves? The queen said so?\" \n",
            "\"No,\" she said, wiping her tears with her sleeve. \"There is a direwolf king on the throne, so I \n",
            "sent him a letter with more direwolves in it.\" \n",
            "Sansa was confused. \"What is this letter?\" she asked. \n",
            "\"To \n",
            "Page 555\n",
            "\n",
            "my queen. There it is, tucked away in a box in her room. I have this large missus with me, but I \n",
            "could not give him the name.\" \n",
            "\"Why would any man want to kill a princess if he couldn't name a kennel?\" \n",
            "\"He is my king. And now my son, I fear.\" \n",
            "\"Then you must kill him,\" she said. \n",
            "The queen nodded. \n",
            "\"Then I'll give him up,\" her brother Tommen announced sharply. He was a handsome young lord, aged \n",
            "Page 555\n",
            "\n",
            "two years and three days, half a horse in his father's best boots, and he would doubtless win or lose \n",
            "the kennel, but Tommen was certain that Maester Aemon would not believe him. \n",
            "\"Kill him now,\" she commanded Joffrey. \n",
            "Joffrey hesitated. \"How?\" he said. \"He has children, Joff. I give him up.\" \n",
            "She took his hand. \"And what will Khal Drogo do? He will tell his lady mother when he comes home, \n",
            "and perhaps even marry her.\" \n",
            "Sansa looked at her brother. \"When he comes home, I will tell the girl that she must not be a whore before \n",
            "her husband.\" \n",
            "\"A whore?\" \n",
            "\"If you refuse my love, my princess has more fish to fry.\" \n",
            "The queen's voice was soft and her brother's look was hard as stone. \"Then I will kill him myself now, \n",
            "or be silent when he comes home.\" \n",
            "\"I am truly frightened of you,\" Joffrey said, horrified. \"I\n",
            "\n",
            "[401 | 961.26] loss=2.12 avg=2.29\n",
            "[402 | 963.53] loss=2.08 avg=2.29\n",
            "[403 | 965.80] loss=2.29 avg=2.29\n",
            "[404 | 968.08] loss=1.95 avg=2.29\n",
            "[405 | 970.35] loss=2.18 avg=2.29\n",
            "[406 | 972.61] loss=2.16 avg=2.29\n",
            "[407 | 974.87] loss=1.71 avg=2.28\n",
            "[408 | 977.15] loss=2.03 avg=2.28\n",
            "[409 | 979.42] loss=1.84 avg=2.27\n",
            "[410 | 981.69] loss=1.86 avg=2.27\n",
            "[411 | 983.95] loss=1.91 avg=2.27\n",
            "[412 | 986.22] loss=2.47 avg=2.27\n",
            "[413 | 988.49] loss=2.22 avg=2.27\n",
            "[414 | 990.77] loss=1.91 avg=2.26\n",
            "[415 | 993.04] loss=2.33 avg=2.26\n",
            "[416 | 995.31] loss=1.90 avg=2.26\n",
            "[417 | 997.59] loss=2.28 avg=2.26\n",
            "[418 | 999.86] loss=2.05 avg=2.26\n",
            "[419 | 1002.12] loss=2.21 avg=2.26\n",
            "[420 | 1004.40] loss=2.06 avg=2.26\n",
            "[421 | 1006.67] loss=1.66 avg=2.25\n",
            "[422 | 1008.94] loss=2.26 avg=2.25\n",
            "[423 | 1011.21] loss=2.16 avg=2.25\n",
            "[424 | 1013.48] loss=1.78 avg=2.24\n",
            "[425 | 1015.75] loss=2.11 avg=2.24\n",
            "[426 | 1018.02] loss=2.11 avg=2.24\n",
            "[427 | 1020.29] loss=1.97 avg=2.24\n",
            "[428 | 1022.57] loss=2.13 avg=2.24\n",
            "[429 | 1024.84] loss=1.85 avg=2.23\n",
            "[430 | 1027.12] loss=1.65 avg=2.23\n",
            "[431 | 1029.39] loss=1.70 avg=2.22\n",
            "[432 | 1031.66] loss=2.04 avg=2.22\n",
            "[433 | 1033.93] loss=1.92 avg=2.22\n",
            "[434 | 1036.20] loss=2.01 avg=2.22\n",
            "[435 | 1038.47] loss=1.74 avg=2.21\n",
            "[436 | 1040.75] loss=1.83 avg=2.21\n",
            "[437 | 1043.02] loss=1.86 avg=2.20\n",
            "[438 | 1045.29] loss=2.06 avg=2.20\n",
            "[439 | 1047.56] loss=1.85 avg=2.20\n",
            "[440 | 1049.83] loss=1.87 avg=2.19\n",
            "[441 | 1052.12] loss=1.99 avg=2.19\n",
            "[442 | 1054.38] loss=1.79 avg=2.19\n",
            "[443 | 1056.66] loss=2.19 avg=2.19\n",
            "[444 | 1058.92] loss=2.01 avg=2.19\n",
            "[445 | 1061.19] loss=2.27 avg=2.19\n",
            "[446 | 1063.47] loss=1.70 avg=2.18\n",
            "[447 | 1065.74] loss=1.74 avg=2.18\n",
            "[448 | 1068.01] loss=1.76 avg=2.17\n",
            "[449 | 1070.28] loss=2.25 avg=2.17\n",
            "[450 | 1072.55] loss=1.94 avg=2.17\n",
            "[451 | 1074.83] loss=1.86 avg=2.17\n",
            "[452 | 1077.10] loss=1.92 avg=2.17\n",
            "[453 | 1079.38] loss=2.03 avg=2.17\n",
            "[454 | 1081.65] loss=1.92 avg=2.16\n",
            "[455 | 1083.92] loss=1.79 avg=2.16\n",
            "[456 | 1086.19] loss=1.79 avg=2.16\n",
            "[457 | 1088.47] loss=1.80 avg=2.15\n",
            "[458 | 1090.74] loss=2.30 avg=2.15\n",
            "[459 | 1093.02] loss=1.83 avg=2.15\n",
            "[460 | 1095.29] loss=1.83 avg=2.15\n",
            "[461 | 1097.56] loss=1.46 avg=2.14\n",
            "[462 | 1099.84] loss=2.05 avg=2.14\n",
            "[463 | 1102.11] loss=1.74 avg=2.13\n",
            "[464 | 1104.37] loss=2.12 avg=2.13\n",
            "[465 | 1106.65] loss=1.77 avg=2.13\n",
            "[466 | 1108.91] loss=1.80 avg=2.13\n",
            "[467 | 1111.19] loss=1.73 avg=2.12\n",
            "[468 | 1113.46] loss=1.62 avg=2.12\n",
            "[469 | 1115.73] loss=1.74 avg=2.11\n",
            "[470 | 1117.99] loss=1.79 avg=2.11\n",
            "[471 | 1120.27] loss=1.81 avg=2.11\n",
            "[472 | 1122.53] loss=1.99 avg=2.11\n",
            "[473 | 1124.80] loss=2.08 avg=2.11\n",
            "[474 | 1127.08] loss=1.93 avg=2.11\n",
            "[475 | 1129.34] loss=1.78 avg=2.10\n",
            "[476 | 1131.61] loss=1.54 avg=2.10\n",
            "[477 | 1133.88] loss=1.77 avg=2.09\n",
            "[478 | 1136.16] loss=1.87 avg=2.09\n",
            "[479 | 1138.44] loss=1.53 avg=2.09\n",
            "[480 | 1140.72] loss=1.83 avg=2.08\n",
            "[481 | 1142.99] loss=1.62 avg=2.08\n",
            "[482 | 1145.25] loss=1.68 avg=2.07\n",
            "[483 | 1147.53] loss=1.56 avg=2.07\n",
            "[484 | 1149.80] loss=1.66 avg=2.06\n",
            "[485 | 1152.07] loss=2.03 avg=2.06\n",
            "[486 | 1154.34] loss=1.93 avg=2.06\n",
            "[487 | 1156.62] loss=1.76 avg=2.06\n",
            "[488 | 1158.89] loss=1.89 avg=2.06\n",
            "[489 | 1161.17] loss=1.72 avg=2.05\n",
            "[490 | 1163.44] loss=1.72 avg=2.05\n",
            "[491 | 1165.71] loss=1.64 avg=2.05\n",
            "[492 | 1167.98] loss=1.79 avg=2.04\n",
            "[493 | 1170.25] loss=1.65 avg=2.04\n",
            "[494 | 1172.54] loss=1.76 avg=2.04\n",
            "[495 | 1174.81] loss=1.78 avg=2.04\n",
            "[496 | 1177.08] loss=1.85 avg=2.03\n",
            "[497 | 1179.35] loss=1.84 avg=2.03\n",
            "[498 | 1181.62] loss=1.61 avg=2.03\n",
            "[499 | 1183.90] loss=2.20 avg=2.03\n",
            "[500 | 1186.16] loss=1.86 avg=2.03\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"What is love beside shrimp?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee0339f-7714-4396-853b-330d7f339587"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is love beside shrimp? What does it mean?\" \n",
            "Ned was not so sure. \"Myrish, I think . . . you mean to tell me what love is?\" \n",
            "\"Oh, perhaps not,\" the prince said. \"Gods, I loved Prince Aemon too, my lords. I love him, and I love \n",
            "everything . . . everything . . . everything . . . everything . . . to do with the gods . . . \n",
            "\"I have nothing against the gods, love me or otherwise.\" \n",
            "\"No.\" Ned said, \"but . . . if you truly mean to tell me what love is, tell me.\" \n",
            "\"I have nothing against the gods, my lord.\" \n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}